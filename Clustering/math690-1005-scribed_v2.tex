\documentclass[12pt]{article}

\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{amsmath, amssymb, amsthm, amsfonts, tikz, algpseudocode}
\usepackage[plain]{algorithm}
\usepackage[framemethod=default]{mdframed}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}
\newtheorem*{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{remark}{Remark}
\newtheorem*{proposition}{Proposition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\eig}{eig}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\vol}{Vol}

%%%% TITLE

\title{Math 690: Topics in Data Analysis and Computation \\
Lecture notes for October 5, 2017}
\date{}

\author{Scribed by Dev Dabke and Andrew Cho}

\begin{document}
\maketitle

\section{Introduction}
The lecture covered the following:
\begin{itemize}
	\item Continue spectral clustering
	\item Graph cut interpretation of spectral clustering
\end{itemize}

\section{Setup Laplacian for spectral clustering}
$\textit{*Insert joke about creating a subroutine for this setup since we start every class with this}$
\\ \\ 
Recall $ W_{n \times n} $ where $ W = W^{\intercal} $ (i.e.\ symmetric) and $ w_{ij} > 0 $.
Additionally, let $ d = \diag{d_1, \ldots, d_n} $ where $ d_i = \sum_{j = 0}^{n} w_{ij} $.
Next, denote
\begin{align*}
  \mathcal{L}_{un} &= D - W \\
  \mathcal{L}_{rw} &= I - D^{-1} W
\end{align*}
and $ P = D^{-1} W $.
\\ \\
Recall that if a graph has $ k $ connected components, then the eigenspace of $ \mathcal{L}_{un} $ associated with eigenvalue $ 0 $ is the span of $ \{ \mathbf{1_{A_{1}}}, \ldots, \mathbf{1_{A_{k}}} \} $.

\section{Exploring $ \mathcal{L}_{rw} $}

\subsection{Redefining Matrices}

What if $ \mathcal{L}_{rw} $ instead of $ \mathcal{L}_{un} $?
We see that $ \mathcal{L}_{rw} = \Psi(1 - \Lambda)\Psi^{\intercal} $ and that $ P = \Psi\Lambda\Psi^{\intercal} $.
Next, we note that $ \Psi^{\intercal} D \Psi = I $.
Now, denoting $ \Psi = [ \varphi_1, \ldots, \varphi_{n} ] $, we can see that $ \varphi_k^{\intercal} D \varphi_l = 0 $ if $ k \neq l $.
Finally, let $ P \phi_k = \lambda_k \phi_l $.

\subsection{Eigenvectors}

If $ f $ is an eigenvector of $ \mathcal{L}_{un} $ with $ \lambda = 0 $, $ \mathcal{L}_{un} f = 0 \dot f $ and $ (D - W)f = 0 $.
If $ f $ is an eigenvector of $ \mathcal{L}_{rw} $ with $ \lambda = 0 $, then $ D^{-1} (D - W) f = 0 $ and $ (D - W)f = 0 $.
\\ \\
Next, we can also write the eigenvectors $ \varphi_{1} = c_1 \mathbf{1_{A_{1}}} + \cdots + c_k \mathbf{1_{A_{k}}} $.
\\ \\
What are the constants?
\[
c_1 = \sqrt{\frac{1}{\sum_{i \in C_1}d_i}}
\]
so $ c_1 = \{ 1, \ldots, n_1 \} $ and $ c_2 = \{ n_1 + 1, \ldots, n_1 + n_2 \} $.

\subsection{The Special Case of $ k = 2$}

We can write $ \widetilde{\varphi_2} = \alpha \varphi_{1}  + \beta \varphi_{2} $. 
From $\textbf{section 3.1}$, we know $ \varphi_k^{\intercal} D \varphi_l = 0 $ if $ k \neq l $ and $ \varphi_k^{\intercal} D \varphi_l = 1 $ if $ k = l $. So for $k=2$ we have, 
\begin{align*}
\widetilde{\varphi_2}^T D \widetilde{\varphi_2} = 1 \implies \alpha^2 + \beta^2 = 1 \\
\widetilde{\varphi_2}^T D \widetilde{\varphi_1} = 0 \implies \widetilde{\varphi_2}^{\intercal}d = 0
\end{align*}
And since $ d = [d_1, \ldots d_n]^{\intercal} $, we have $ \alpha(\varphi_{1}^Td) + \beta(\varphi_{2}^Td) = 0$ and $\alpha\sqrt{\sum_{i \in C_1}d_i}+\beta\sqrt{\sum_{i \in C_2}d_i} = 0$.
\\ \\
Now, $ \alpha $ and $ \beta $ should have different signs because $ \widetilde{\varphi_2} = \alpha c_1 \mathbf{1}_{A_1} + \beta c_2 \mathbf{1}_{A_2} $. In other words, 

\[
\widetilde{\varphi_2} (i) =
\begin{cases}
  \alpha c_1 & if i \in c_1 \\
  \beta c_2 & if i \in c_2
\end{cases}
\]
Therefore, we can use the signs of $\alpha$ and $\beta$ to get the cluster or in general, we don't have to necessarily cut at zero per say but can tune to any cut-off other than zero. 

\subsection{Deciding $ k $ for spectral clustering}

We can use the spectral gap heuristic, which tells us that the first $ k $ eigenvalues will be close to one another, and then there will be a ``gap'' after $ k $.
We could use the log values of the ordered eigenvalues can provide some estimate as to where this gap could be.

\section{Graph Cut}

\begin{remark}
Here we take a graph cut point of view in understanding spectral clustering and why it works. The following is the introduction given by the 2007 Luxburg paper. This wasn't necessarily part of the lecture discussion, but we thought it helps understand the section better.
\end{remark}

\begin{quote}
The intuition of clustering is to separate points in different groups according to their similarities. For data given in form of a similarity graph, this problem can be restated as follows: we want to find a partition of the graph such that the edges between different groups have a very low weight (which means that points in different clusters are dissimilar from each other) and the edges within a group have high weight (which means that points within the same cluster are similar to each other). In this section we will see how spectral clustering can be derived as an approximation to such graph partitioning problems (Luxburg '07 pg 8).
\end{quote}

\subsection{Getting Started}

\begin{definition}[Cut]

Let $ A \cap B = \emptyset $ where $ A, B \subset V = \{ 1, \ldots, n \} $.
Then a cut $ W(A, B) $ is defined as
\[
W(A, B) = \sum_{i \in A, j \in B} w_{ij}
\]

\end{definition}

\begin{definition}[Volume Set]

$ \vol(A) = \sum_{i \in A} d_i $ is the sum of degrees.

\end{definition}

\begin{definition}[Normalized Cut Partition]
Let $ C = \{ C_1, \ldots, C_k \} $ and denote
\[
NCut(C) = \frac{ \sum_{l = 1}^k W(C_1, C_l^c) }{ \vol{C_l} }
\]

\end{definition}

\begin{proposition}
  \begin{align*}
    NCut(C) &= \sum_{l = 1}^{k} \varphi_l^{\intercal} L \varphi_{l} \\
    \varphi_l &= c_{l} \mathbf{1_{c_{l}}} & l = 1, \ldots, k \\
    c_l &= \frac{1}{\sqrt{\vol{C_l}}}
  \end{align*}
\end{proposition}

\begin{proof}
	\begin{align*}
		\varphi_l^{\intercal} L \varphi_{l} &= \frac{1}{2} \sum_{ij} w_{ij}(\varphi_l(i) - \varphi_l(j))^2 \\
		&= \sum_{i \in C_l, j \in C_l^c} w_{ij} (\frac{1}{Vol(C_l)} - 0)^2 \\
		&= \sum_{i \in C_ll, j \in C_l^c} w_{ij} \frac{1}{Vol(C_l)} \\
		&= \frac{W(C_l,C_l^c)}{Vol(C_l)}
	\end{align*}
\end{proof}

\subsection{Problem of Min NCut}

Let $ H $ be the matrix such that $ H = [\varphi_1, \ldots, \varphi_k] $.
Then, the minimum $ NCut $ over the $ C $s is the trace of $ H^{\intercal} L H $.
\\ \\
The spectral minimum $ NCut $ is defined to be the above trace, but where $ H^{\intercal} D H = I_{k} $.
\begin{proposition}
  The solution of the spectral minimum $ NCut $ is the first $ k $ eigenvectors of $ \mathcal{L}_{rw} $ normalized.
\end{proposition}

\subsection{Next class}

We will discuss which Laplacian to use between $\mathcal{L}_{un}$, $\mathcal{L}_{rw}$, and $\mathcal{L}_{sym}$ next class.

\end{document}
