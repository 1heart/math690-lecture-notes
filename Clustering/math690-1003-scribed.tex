\documentclass[12pt]{article}

\usepackage[a4paper,margin=2cm]{geometry}
\usepackage{amsmath, amssymb, amsthm, amsfonts, tikz, algpseudocode}
\usepackage[plain]{algorithm}
\usepackage[framemethod=default]{mdframed}
\usepackage{hyperref}

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{claim}{Claim}
\newtheorem*{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem*{remark}{Remark}
\newtheorem*{proposition}{Proposition}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\eig}{eig}

%%%% TITLE

\title{Math 690: Topics in Data Analysis and Computation \\
Lecture notes for October 3, 2017}
\date{}

\author{Scribed by Dev Dabke and Andrew Cho}

\begin{document}
\maketitle

\section{Introduction}
The lecture covered the following:
\begin{itemize}
	\item Discuss KK10 paper on consistency of Lloyd's k-means algorithm
	\item Start spectral clustering
\end{itemize}

\section{k-means clustering}
\label{sec:kmeans}

\subsection{Lloyd's algorithm}
\label{subsec: lloyd}

Recall Lloyd's algorithm from last lecture that given some data points as input $ \{ x_i \}^n_{i = 1} $, we want to search for $ k $ clusters $ C = \{ C_1, \ldots, C_k \} $ with ``centers'' $ \mu = \{ \mu_1, \ldots, \mu_k \} $.
\\ \\
Depending on our constraints, also recall we can choose different objective functions and have different interpretations of what these clusters represent.
\begin{align*}
  \min_{C, \mu} & \sum_{l = 1}^k \sum_{i \in C_{l}} \| x_i - \mu_{l} \|_2^2 & \text{k means} \\
  \min_{C, \mu} & \sum_{l = 1}^k \sum_{i \in C_{l}} \| x_i - \mu_{l} \|_{1} & \text{k medians} \\
  \min_{C, \mu} & \sum_{l = 1}^k \sum_{i \in C_{l}} \| x_i - \mu_{l} \|_2 & \text{$ L_2 $ - $ L_1 $ norm}
\end{align*}
We can either choose initial seeds by randomly selecting k points or by singular value decomposition (SVD). Ultimately, we are interested in knowing if these results are \textit{consistent}.
Can these seeding methods lead to the true centroids?

\subsection{Strong Law of Large Numbers}
\label{subsec:slln}

\[
\mathcal{L}_n (\mu) = \int \| x - \mu \|^2 d P_n(x)
\]
where
\[
d P_n (x) = \frac{1}{n} \sum_{i = 1}^n \delta_{x_i} (x) dx
\]
which is the empirical measure of the data.
\\ \\
Where $ A = \{ a_1, \ldots, a_n \} $ for all $ x \in \mathbb{R} $, we define $ \| x - A \| $ to be
\[
\| x - A \| = \min_{1 \leq i \leq n} \| x - a_i \|
\]
Now, note that as $ n \to \infty $ that
\[
\mathcal{L} (\mu) = \int \| x - \mu \|^2 dP(x)
\]
and that
\[
\min_{\mu} \mathcal{L}_n (\mu) \to \min_{\mu} \mathcal{L} (\mu)
\]

\subsection{Consistency of Lloyd}
\label{subsec:consistency}

\textbf{Ref} (Kumar, Amit, and Ravindran Kannan 2010)

If we satisfy some requirements of ``separation'' to provide some bound for misclassification, assuming some true centroid partition.
\\ \\
$ \| \mu^{*}_{k} - \mu^{*}_{l} \| $ if $ k \neq l $ needs to be bigger than a gap $ \Delta_{kl} $.
\\ \\
\[
\Delta_{kl} > c \frac{\sigma_l}{\sqrt{w_{min}}} \hspace{1cm} w_{min} = \min_{l} \frac{|C_l^{*}|}{n}
\]
$w_{min}$ denotes proportions of points in cluster $l$. Letting $ \sigma_k^2 $ be the variance of the data in the $ k^{th} $ cluster and if we can assume that $ \sigma_k \approx \sigma_l $, then we know that misclassification error from SVD initialization occurs less than some $ \epsilon * n $ with high probability.

\subsection{In Practice}
\label{subsec:inpractice}

There are some practical considerations for k-means.
\begin{enumerate}
  \item what is $ k $?
  \item how to choose an initial seed
  \item some cases of k-means fails
  \begin{enumerate}
    \item $ \sigma_k $ might be too large compared to separation
    \item clusters might be too small (i.e.\ cluster sizes may not be balanced)
    \item cluster might be convex and piecewise can't do concave (Voronoi)
  \end{enumerate}
\end{enumerate}

\section{Spectral clustering}
\label{sec:spectral}

\subsection{Graph Laplacian}

Given some data $ \{ x_i \}_{i = 1}^n $ and $ k $, we want to
\begin{enumerate}
  \item Build positive-definite, symmetric affinity matrix $ W_{n \times n} $ by $ k $ nearest neighbors, $ \epsilon $ - neighbor, or the Gaussian kernel $ W_{ij} = e^{- \frac{\| x_i - x_j \|^2}{\epsilon}} $.
  \item Consider the eigenvalue decomposition of the graph Laplacian $ \mathcal{L} $.
  Note that
  \begin{align*}
    L_{un} &= D - W  & \text{unnormalized} \\
    L_{rw} &= D^{-1} (D - W) & \text{Shi-Malik '00} \\
    L_{sym} &= I - D^{-1/2} W D^{-1/2} & \text{Ng-Jordan-Weiss '02}
  \end{align*}
  \item Apply k means to $ \Psi = [\varphi_i, \ldots, \varphi_k]_{n \times k} $ and denote $ y_i $ as the $ i^{th} $ row of $ \Psi $.
\end{enumerate}

\begin{remark}
  If $ k = 2$, you can use truncation because the first eigenvalue is constant. Thus, you can use sign($\Psi_2$) to indicate clustering.
\end{remark}

\begin{definition}[Connected Components]

Let $ \mathcal{G} = (V, E) $ such that on each edge $ (i, j) \in \mathcal{G} $ that $ w_{ij} > 0 $.
If node $ i $ is connected to node $ j $, there is a path from $ i $ to $ j $.
So then, set $ A $ is a connected component if every pair of $ i $ and $ j $ is connected and $ A $ is the maximum set that satisfies this condition to preserve connectivity.

\end{definition}

\begin{proposition}[Eigenspace of $ \lambda = 0 $ of $\mathcal{L}$]

Suppose the graph has $ k $ connected components $ A_{1}, \ldots, A_{k} $.
Then the eigenspace of $ \lambda = 0 $ of $ \dim{k} $ is spanned by
\[
\{ \mathbf{1}_{A_{1}}, \ldots, \mathbf{1}_{A_{k}} \}
\]
where
\[
\mathbf{1}_{A_{i}} (i) =
\begin{cases}
  1 & i \in A \\
  0 & \text{otherwise}
\end{cases}
\]

\end{proposition}

\textbf{Proof}
Suppose $\mathbf{f}$ is an eigenvector with  $\lambda = 0$. So $\mathcal{L}\textbf{f} = \textbf{0}$ and $\textbf{f}^T\mathcal{L}\textbf{f} = \textbf{0}$. We also know $\textbf{f}^T\mathcal{L}\textbf{f} = \textbf{0} = \frac{1}{2} \sum_{(i,j)}w_{i,j}(f_i-f_j)^2$ and $\textbf{f}^T\mathcal{L}\textbf{f} = \textbf{0} \Leftrightarrow f_i=f_j$ whenever $w_{i,j} \textgreater 0$. 
Thus, $\textbf{f}$ is a piecewise constant in each of the connected components. Meanwhile, for each $v \in$ span $\{\mathbf{1}_{A_{1}}, \ldots, \mathbf{1}_{A_{k}} \}, \mathbf{v}^T \mathcal{L} \mathbf{v} = \mathbf{0}$. 
\\ \\ 
\textbf{Exercise:}
\begin{itemize}
	\item Does this generalize to $\mathcal{L}_{rw}$ and $\mathcal{L}_{sym}$?
	\item What about consistency of spectral clustering?
\end{itemize}

\end{document}
